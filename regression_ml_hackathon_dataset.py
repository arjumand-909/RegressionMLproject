# -*- coding: utf-8 -*-
"""Regression_Ml_Hackathon_Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AQCMRaQJS_Hec34CMcoASSx8hM6Kv81V

# Arjumand Afreen Tabinda
# 411210
```

#‚úÖ Dataset Overview (insurance.csv)

1Ô∏è‚É£ Shape of the dataset (rows, columns):
There are 1338 rows and 7 columns

‚úÖ Step 1: **Analyze the dataset (insurance.csv)**
‚úÖ Step 2: **Give a clear Dataset Overview**

---

## ‚úÖ Dataset Overview (insurance.csv)

**1Ô∏è‚É£ Shape of the dataset (rows, columns):**
There are **1338 rows** and **7 columns**

**2Ô∏è‚É£ Columns:**


| Column   | Type   | Description                                                |
| -------- | ------ | ---------------------------------------------------------- |
| age      | int    | Age of the person                                          |
| sex      | object | Gender (male/female)                                       |
| bmi      | float  | Body Mass Index                                            |
| children | int    | Number of dependents                                       |
| smoker   | object | Smoker or not (yes/no)                                     |
| region   | object | Residential area (northeast/northwest/southeast/southwest) |
| charges  | float  | Medical insurance cost (Target Variable)                   |

**3Ô∏è‚É£ Data Types:**

* Numerical: age, bmi, children, charges
* Categorical: sex, smoker, region

**4Ô∏è‚É£ Missing Values:**
‚úî No missing values in any column (all 0 nulls)

**5Ô∏è‚É£ Basic Stats (Numerical Columns):**
(age, bmi, children, charges)
‚Üí mean, min, max, std available in describe()

**6Ô∏è‚É£ Target Variable:**
üéØ `charges` = Medical cost (this is what we will predict using ML)

**7Ô∏è‚É£ Quick Observations:**

* `smoker` likely VERY important (smokers pay much higher)
* `bmi` may correlate with charges
* `region` might have slight variation
* `charges` is right-skewed (we may need log transform later)

---

# üéØ **Library Loading and Preprocessing Overview**
"""

#loading need libraries
import numpy as np
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats

#preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

"""```

# ‚úÖ **Purpose**:

This block loads all the **essential libraries needed for data analysis, visualization, and statistics.**

# ‚úÖ **Explanation (line by line):**

| Code                              | Explanation                                                                                   |
| --------------------------------- | --------------------------------------------------------------------------------------------- |
| `import numpy as np`              | Used for fast numerical calculations, arrays, and math functions.                             |
| `import seaborn as sns`           | Used for beautiful and advanced statistical data visualizations (heatmaps, boxplots, etc.).   |
| `import pandas as pd`             | Used to load, clean, explore, and manipulate tabular data using DataFrames.                   |
| `import matplotlib.pyplot as plt` | Basic plotting library to create charts and customize visualizations.                         |
| `from scipy import stats`         | Provides statistical tools (like normality tests, correlations, z-scores, outlier detection). |

---

```python
#preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
```

### ‚úÖ Purpose:

This block loads tools required to **prepare data for machine learning.**

### ‚úÖ Explanation (line by line):

| Code                                                   | Explanation                                                                                                                    |
| ------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------ |
| `from sklearn.model_selection import train_test_split` | Splits the dataset into training and testing sets to evaluate model performance safely.                                        |
| `from sklearn.preprocessing import LabelEncoder`       | Converts categorical text labels (e.g., male/female) into numeric values (0/1) so machine learning models can understand them. |

---

# **Loading the Dataset**
"""

import pandas as pd
train = pd.read_csv('/content/insurance.csv')
train

"""# üìå Step-by-Step Breakdown of Loading the Dataset

---

# üìå Step-by-Step Breakdown of Loading the Dataset

| Step                           | Explanation                                                                                                                                                                                      |
| ------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **1. Reading the CSV File**    | The function `pd.read_csv()` (from the Pandas library) is used to load data from a CSV (Comma-Separated Values) file into Python.                                                                |
| **2. File Path Specification** | The path `'/content/train.csv'` indicates the location of the dataset. This path may differ depending on whether you‚Äôre using local storage, Google Colab, or Jupyter Notebook.                  |
| **3. Storing as a DataFrame**  | The loaded data is saved in a variable named `train`, which becomes a Pandas DataFrame. A DataFrame is a table-like structure with rows and columns, making data easy to explore and manipulate. |
| **4. Displaying the Data**     | Writing `train` (or using `train.head()`) displays the first few rows of the dataset, helping us confirm that the data was loaded correctly and understand its structure.                        |

---

# ‚úÖ Importance of This Step

* Loads the raw dataset into memory for analysis.
* Serves as the foundation for all further steps (cleaning, visualization, modeling).
* Ensures data is organized in a DataFrame, which simplifies manipulation.
* Without this step, no exploration, preprocessing, or model training can happen.

---

# ‚öôÔ∏è Example Output (Preview of Data)

| age | gender | bmi   | children | smoker | region    | charges   |
| --- | ------ | ----- | -------- | ------ | --------- | --------- |
| 19  | female | 27.9  | 0        | yes    | southwest | 16884.924 |
| 18  | male   | 33.77 | 1        | no     | southeast | 1725.552  |
| 28  | male   | 33.0  | 3        | no     | southeast | 4449.462  |

---

# **üîç Checking for Missing Values**
"""

train.isnull().sum()

"""## üîπ Step-by-Step Explanation

| Step | Description |
|------|--------------|
| **1. Purpose of `isnull()`** | The function **`isnull()`** checks each cell in the DataFrame and returns **True** if the value is missing (NaN), otherwise **False**. |
| **2. Purpose of `sum()`** | When we apply **`.sum()`**, it counts the total number of **True** values (i.e., missing values) in each column. |
| **3. Combined Use** | Therefore, **`train.isnull().sum()`** gives us the **total number of missing values per column** in the dataset. |

---

## üìä Output Example

| Column | Missing Values |
|---------|----------------|
| age | 0 |
| gender | 0 |
| bmi | 0 |
| children | 0 |
| smoker | 0 |
| region | 0 |
| charges | 0 |

---

## üß© Interpretation

‚úÖ The output shows **0 missing values** in all columns.  
‚úÖ This means our dataset is **clean and complete**, with **no missing entries**.  
‚úÖ Hence, **no data imputation or cleaning** is required at this stage.  

---

## üí° Why This Step is Important

- Ensures **data quality and integrity** before modeling.  
- Detects **missing or corrupted data** early.  
- Prevents **errors or biases** during machine learning model training.  

‚úÖ **In this case:** *No missing values found ‚Äî data is ready for preprocessing and exploration!*

# üìè Understanding `train.shape`
"""

train.shape

"""## üîπ Step-by-Step Explanation

| Step | Description |
|------|--------------|
| **1. Purpose of `shape`** | The **`.shape`** attribute in Pandas returns the **dimensions of the DataFrame** as a tuple ‚Äî `(rows, columns)`. |
| **2. What It Represents** | The first value indicates the **number of rows (data records)**, and the second value indicates the **number of columns (features or attributes)**. |
| **3. How It Helps** | It helps us understand the **size of the dataset**, which is crucial for planning data analysis, visualization, and model training. |

---

## üìä Example Output

```python
(1338, 7)
```

## üß© Interpretation

| Element | Meaning |
|----------|----------|
| **1338** | Total number of rows ‚Äî represents the total number of individuals or records in the dataset. |
| **7** | Total number of columns ‚Äî represents the different features or attributes (like age, gender, bmi, etc.). |

---

## üí° Why This Step is Important

- Helps in **understanding dataset size and structure**.  
- Useful for **data validation** and ensuring the dataset loaded correctly.  
- Provides quick insight into **how much data** we have for analysis and model training.  

‚úÖ **In this case:** The dataset has **1338 rows** and **7 columns**, which makes it **suitable for building a predictive model.**

#  Details `train.info()`
"""

train.info()

"""---

# Details `train.info()`

| Step                                         | Explanation                                                                                                                                          |
| -------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1. Function Purpose**                      | `train.info()` is used to display the basic structural information of the dataset. It gives a quick summary of the DataFrame.                        |
| **2. Total Rows & Columns**                  | It shows how many rows and columns are in the dataset, helping us understand the dataset size.                                                       |
| **3. Column Names & Data Types**             | It lists each column along with its data type (int, float, object, etc.), which helps us know how data is stored.                                    |
| **4. Non-Null Count (Missing Values Check)** | It tells how many non-null values each column has. If the non-null count is less than the total number of rows, then that column has missing values. |
| **5. Memory Usage**                          | It shows how much memory the DataFrame consumes. This is useful for optimizing performance in large datasets.                                        |

---

# ‚úÖ Why This Step Matters

* Helps detect **missing data** early.
* Tells us which columns are **categorical or numerical**.
* Guides us in **choosing correct preprocessing techniques**.
* Ensures we understand the **structure of the dataset** before applying any transformations.

---

# **üîç Identifying and Eliminating Duplicate Entries**
"""

train.duplicated().sum()
train.drop_duplicates(inplace=True)

"""---

# üî∏ Detailed Breakdown

| Step                               | Explanation                                                                                                                          |
| ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| **1. Role of `duplicated()`**      | The `duplicated()` function scans the DataFrame to identify repeated rows. It marks duplicates as `True` and unique rows as `False`. |
| **2. Using `sum()`**               | Applying `.sum()` counts how many `True` values exist, which gives the total number of duplicate records.                            |
| **3. Role of `drop_duplicates()`** | The `drop_duplicates()` function removes all repeated rows from the dataset, keeping only the first instance of each record.         |
| **4. `inplace=True`**              | This argument applies the changes directly to the original DataFrame without creating a new one.                                     |

---

# üìå Sample Result

```python
train.duplicated().sum()
```

**Output:** `1`
‚úÖ This means there was **1 duplicate row** in the dataset.

After running:

```python
train.drop_duplicates(inplace=True)
```

‚úÖ The duplicate entry was successfully removed.

---

# Integer Columns
"""

int_col = train.select_dtypes(include=['int64']).columns.tolist()
train[int_col]

"""

---

# üî∏ Step-by-Step Breakdown

| Step                             | Explanation                                                                                                       |
| -------------------------------- | ----------------------------------------------------------------------------------------------------------------- |
| **1. Role of `select_dtypes()`** | The `select_dtypes()` method in Pandas allows us to pick columns from the DataFrame based on specific data types. |
| **2. `include=['int64']`**       | This argument selects only those columns whose data type is `int64` (i.e., integer columns).                      |
| **3. `.columns.tolist()`**       | Retrieves just the column names (not the values) and converts them into a Python list for easier usage.           |
| **4. `train[int_col]`**          | Displays the subset of the DataFrame that contains only the integer-type columns.                                 |

---

# üìå Example Output

| Column Name | Description                        |
| ----------- | ---------------------------------- |
| age         | Represents the individual's age.   |
| children    | Number of dependents/children.     |
| region_code | Encoded region or area identifier. |

*(Column names may differ depending on the dataset.)*

---

"""

float_col = train.select_dtypes(include=['float64']).columns.tolist()
train[float_col]

"""## üîπ Step-by-Step Explanation

| Step | Description |
|------|--------------|
| **1. Purpose of select_dtypes()** | The `select_dtypes()` function in Pandas is used to select columns in a DataFrame based on their data type. |
| **2. include=['float64']** | This parameter filters columns that have the data type **float64** (i.e., decimal or continuous numeric values). |
| **3. .columns.tolist()** | Extracts only the column names (not the data) and converts them into a **Python list** for easy access and reference. |
| **4. train[float_col]** | Displays only those columns from the dataset that contain **floating-point (decimal)** data. |

---

## üìä Example Output

| Column Name | Description |
|--------------|--------------|
| bmi | Represents the body mass index (weight-to-height ratio). |
| charges | The medical insurance cost billed to the patient. |

*(Example columns may vary depending on dataset structure.)*

---

## üß© Interpretation

| Element | Meaning |
|----------|----------|
| **float_col** | A list containing names of all float-type (decimal) columns. |
| **train[float_col]** | Displays a filtered DataFrame showing only float columns. |

---

## üí° Why This Step is Important

- Helps focus on **continuous numeric data**, which is essential for statistical analysis and regression modeling.  
- Useful for performing **scaling, normalization, or correlation** analysis on float-type columns.  
- Makes it easier to detect patterns or outliers in continuous features.  
- Ensures that all numerical attributes are handled appropriately before model training.  

‚úÖ **In this case:** The code extracts all floating-point columns from the dataset, allowing targeted analysis and preprocessing of continuous numerical features.

"""

cat_col = train.select_dtypes(include=['object']).columns.tolist()
train[cat_col]

"""

---

# üîª Detailed Explanation

| Step                                 | Explanation                                                                                                                  |
| ------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------- |
| **1. Function of `select_dtypes()`** | The `select_dtypes()` method in Pandas helps us pick specific columns from a DataFrame based on their data type.             |
| **2. `include=['object']`**          | This argument selects only those columns whose data type is `object`, which typically represents text or categorical values. |
| **3. `.columns.tolist()`**           | Extracts just the column names (not the data itself) and converts them into a Python list for easy handling.                 |
| **4. `train[cat_col]`**              | Displays only the columns from the dataset that contain categorical (non-numeric) information.                               |

---

# üìã Sample Output

| Column Name | Description                                                 |
| ----------- | ----------------------------------------------------------- |
| gender      | Shows whether the person is male, female, etc.              |
| smoker      | Indicates if the person is a smoker or not.                 |
| region      | Specifies the geographic area (e.g., northeast, southeast). |

*(Actual column names may differ depending on the dataset.)*

---

# üß† Key Insight

| Element            | Meaning                                                                  |
| ------------------ | ------------------------------------------------------------------------ |
| **cat_col**        | A list containing all column names with categorical (object-type) data.  |
| **train[cat_col]** | A filtered version of the DataFrame that shows only categorical columns. |

---

"""

# Check the data types of all columns
train.dtypes

# Identify numeric columns that might contain categorical data
for col in train.columns:
    unique_values = train[col].nunique()
    if train[col].dtype in ['int64', 'float64'] and unique_values < 10:
        print(f"‚ö†Ô∏è Possible Categorical Column (Stored as Numeric): {col} | Unique Values: {unique_values}")

# Identify object columns that might contain numeric data
for col in train.select_dtypes(include=['object']).columns:
    if train[col].str.isnumeric().any():
        print(f"‚ö†Ô∏è Possible Numeric Column (Stored as Object): {col}")

"""---

# ‚úÖ Purpose of the Code

This code helps us **check data types** and **detect columns that may be incorrectly stored** (e.g., numbers stored as text or categories stored as numbers).
It ensures **proper data preprocessing** before modeling.

---

# üß† Step-by-Step Explanation

### ‚úÖ 1. Check Data Types of All Columns

```python
train.dtypes
```

**Explanation:**
Displays the data type (int, float, object, etc.) of each column in the dataset.
This helps us understand how each variable is stored.

---

### ‚úÖ 2. Find Numeric Columns That Might Actually Be Categorical

```python
for col in train.columns:
    unique_values = train[col].nunique()
    if train[col].dtype in ['int64', 'float64'] and unique_values < 10:
        print(f"‚ö†Ô∏è Possible Categorical Column (Stored as Numeric): {col} | Unique Values: {unique_values}")
```

**Explanation:**

* Loops through all columns.
* Counts unique values in each column.
* If a column is **numeric (int/float)** but has **very few unique values (e.g., 0/1 or 1‚Äì5)**
  üëâ It is likely **categorical in nature** (e.g., gender, yes/no).
* Prints a warning showing such columns.

‚úÖ **Why important?**
Because we should encode them properly (e.g., One-Hot Encoding), not treat them as continuous numeric data.

---

### ‚úÖ 3. Find Object (string) Columns That Might Actually Be Numeric

```python
for col in train.select_dtypes(include=['object']).columns:
    if train[col].str.isnumeric().any():
        print(f"‚ö†Ô∏è Possible Numeric Column (Stored as Object): {col}")
```

**Explanation:**

* Selects only object (string/text) columns.
* Checks if any value in that column contains only numbers.
* If yes ‚Üí It might be a **numeric column stored as text** due to formatting or import issues.
* Prints a warning with such column names.

‚úÖ **Why important?**
These columns should be converted to numeric type to allow mathematical operations or scaling.

---

# üéØ Overall Benefit

This code helps us:
‚úÖ Detect misclassified data types
‚úÖ Decide correct preprocessing/encoding
‚úÖ Improve model performance by fixing data quality issues

---

##  Visualizing Target Variable Distribution
"""

from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

plt.subplots(figsize=(9,9))

# Using distplot (as you want)
sns.distplot(train['charges'], fit=stats.norm)

# Manually using your values
mu = 9.10
sigma = 0.92

# Correct legend with raw string
plt.legend([r'Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f})'.format(mu, sigma)], loc='best')

plt.ylabel('Frequency')
plt.title('Distribution of Charges with Normal Fit')
plt.show()

"""

---

# üéØ Purpose of the Code

This code visualizes the distribution of the **`charges`** column and fits a **Normal Distribution curve** on top of it.
This helps us understand whether the data follows a normal pattern, which is important for statistical analysis and certain machine learning models.

---

# üîç Step-by-Step Explanation

| Step                                                  | Explanation                                                                                                                         |
| ----------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| **1. Import Libraries**                               | `matplotlib`, `seaborn`, and `scipy.stats` are imported for plotting and statistical fitting.                                       |
| **2. Set Figure Size**                                | `plt.subplots(figsize=(9,9))` defines the size of the plot to make the visualization clear and readable.                            |
| **3. Plot Distribution**                              | `sns.distplot(train['charges'], fit=stats.norm)` creates a histogram with a KDE curve and fits a normal distribution over the data. |
| **4. Use Manual Mean (Œº) and Standard Deviation (œÉ)** | Instead of calculating automatically, we manually assign `mu = 9.10` and `sigma = 0.92` based on user-provided values.              |
| **5. Add Legend**                                     | `plt.legend(...)` displays the normal distribution equation with Œº and œÉ values on the graph for better interpretation.             |
| **6. Label Axis and Title**                           | `plt.ylabel('Frequency')` labels the Y-axis, and `plt.title(...)` explains what the chart represents.                               |
| **7. Display Plot**                                   | `plt.show()` renders the final visualization.                                                                                       |

---

# ‚úÖ Why This Step Is Important?

‚úî Helps visualize the shape of the data
‚úî Shows whether data is normally distributed or skewed
‚úî Useful for detecting outliers
‚úî Supports model selection and statistical testing

---


"""

train['charges']

"""

---

# üéØ Purpose of `train['charges']`

The purpose of `train['charges']` is to **access the ‚Äúcharges‚Äù column** from the DataFrame named `train`.
This allows us to work specifically with the values of the **charges** feature (medical insurance cost) for further analysis or operations.

---

# üîç Explanation

| Part               | Meaning                                                                        |
| ------------------ | ------------------------------------------------------------------------------ |
| `train`            | The Pandas DataFrame that contains the dataset.                                |
| `['charges']`      | Column selection syntax to extract only the **charges** column.                |
| `train['charges']` | Returns a **Pandas Series** containing all values from the **charges** column. |

---

# ‚úÖ Why This is Useful

Using `train['charges']`, we can:

* View the data in that column
* Perform statistical calculations (mean, median, std, etc.)
* Create visualizations (histograms, boxplots, distributions)
* Use it as a target variable (for regression models)

---


"""

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats

# 1. Apply log transformation to reduce skewness
train['charges'] = np.log1p(train['charges'])

# 2. Plot distribution after transformation
plt.subplots(figsize=(12,9))
sns.distplot(train['charges'], fit=stats.norm)

# 3. Manually set your desired mean and std deviation
mu = 9.10
sigma = 0.92

# 4. Add legend with your custom values
plt.legend([r'Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f})'.format(mu, sigma)], loc='best')

plt.ylabel('Frequency')
plt.title('Log-Transformed Charges Distribution with Normal Fit')
plt.show()

"""---

# üéØ Purpose of the Code

This code applies a **log transformation to the `charges` column to reduce skewness** and then **visualizes the transformed distribution** along with a **manually defined normal distribution (mean = 9.10, std = 0.92)** for comparison.

---

# üîç Step-by-Step Explanation

| Step                                              | Explanation                                                                                                                                                                        |
| ------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1. Importing Libraries**                        | `numpy`, `seaborn`, `matplotlib`, and `scipy.stats` are imported to perform transformation, visualization, and statistical operations.                                             |
| **2. Log Transformation (`np.log1p`)**            | `train['charges'] = np.log1p(train['charges'])` applies log(1 + x), which reduces right skewness and makes the data more normally distributed. This is often done before modeling. |
| **3. Plot Distribution**                          | `sns.distplot(...)` creates a histogram and KDE curve to show the shape of the transformed data and overlays a fitted normal distribution curve.                                   |
| **4. Manually Set Mean and Standard Deviation**   | Instead of calculating them, we manually assign `mu = 9.10` and `sigma = 0.92` to match a desired reference or comparison.                                                         |
| **5. Display Normal Distribution Info in Legend** | The legend shows the values of Œº and œÉ, helping us visually compare the data with a normal curve using our custom values.                                                          |
| **6. Add Axis Label and Title**                   | `plt.ylabel()` and `plt.title()` make the chart easy to understand by describing what the visualization represents.                                                                |
| **7. Show Plot**                                  | `plt.show()` displays the final graph.                                                                                                                                             |

---

# ‚úÖ Why This is Useful

‚úî Reduces skewness for better statistical behavior
‚úî Helps determine if log-transformed data is closer to normal
‚úî Visual comparison with a standard normal curve
‚úî Important for regression models and assumptions

---

# üö® Detect Outliers in Charges
"""

plt.figure(figsize=(10,5))
sns.boxplot(x=train['charges'], color='lightcoral')
plt.title('Boxplot of Charges', fontsize=16)
plt.xlabel('Charges')
plt.show()

"""---

# üì¶ Boxplot Visualization of Charges

## üéØ Purpose of the Code

The purpose of this code is to **visualize the distribution of the `charges` column using a boxplot** in order to **detect outliers, understand data spread, and observe the central tendency** of the charges in the dataset.

---

## üîç Step-by-Step Explanation

| Step                                                         | Explanation                                                                                                                                                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1. `plt.figure(figsize=(10,5))`**                          | Sets the size of the figure to 10 (width) √ó 5 (height) for better readability.                                                                                                  |
| **2. `sns.boxplot(x=train['charges'], color='lightcoral')`** | Creates a **boxplot** of the `charges` data. Boxplots show median, quartiles, data spread, and highlight outliers. The color `lightcoral` is used to improve visual appearance. |
| **3. `plt.title('Boxplot of Charges', fontsize=16)`**        | Adds a descriptive title to the chart with a font size of 16 to make it clear and readable.                                                                                     |
| **4. `plt.xlabel('Charges')`**                               | Labels the x-axis so the viewer understands what data the boxplot represents.                                                                                                   |
| **5. `plt.show()`**                                          | Displays the final boxplot.                                                                                                                                                     |

---

## ‚úÖ Why This Visualization is Important

‚úî Quickly detects **outliers (extremely high or low values)**
‚úî Shows **median (central value) of charges**
‚úî Displays **data spread and variability**
‚úî Helps in deciding **whether to remove or treat outliers** before modeling

---

# **3Ô∏è‚É£ Violin Plot by Gender**
"""

plt.figure(figsize=(8,6))
sns.violinplot(x='sex', y='charges', data=train, palette='Set2')
plt.title('Distribution of Charges by Gender', fontsize=16)
plt.show()

"""---

# üéª Violin Plot ‚Äì Charges Distribution by Gender

## üéØ Purpose of the Code

The purpose of this code is to **compare the distribution of medical charges between different genders** using a **violin plot**. This helps us understand how charges vary for males and females, including spread, density, and median values.

---

## üîç Step-by-Step Explanation

| Step                                                                      | Explanation                                                                                                                                                                                                                      |
| ------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1. `plt.figure(figsize=(8,6))`**                                        | Sets the size of the plot to 8 (width) √ó 6 (height) for clear visualization.                                                                                                                                                     |
| **2. `sns.violinplot(x='sex', y='charges', data=train, palette='Set2')`** | Creates a violin plot where:<br>‚Ä¢ `x='sex'` ‚Üí Gender categories on the x-axis<br>‚Ä¢ `y='charges'` ‚Üí Charges on the y-axis<br>‚Ä¢ `data=train` ‚Üí Uses the dataset<br>‚Ä¢ `palette='Set2'` ‚Üí Applies a visually appealing color palette |
| **3. `plt.title('Distribution of Charges by Gender', fontsize=16)`**      | Adds a descriptive title to explain what the chart represents.                                                                                                                                                                   |
| **4. `plt.show()`**                                                       | Displays the final visualization.                                                                                                                                                                                                |

---

## ‚úÖ Why This Visualization is Useful

‚úî Shows the **full distribution** (not just summary statistics)
‚úî Displays **median and spread** of charges per gender
‚úî Reveals **density and shape** of data distribution
‚úî Helps check if one gender generally has **higher or lower charges**

---

# Scatter Plot: Age vs Charges
"""

plt.figure(figsize=(10,6))
sns.scatterplot(x='age', y='charges', hue='smoker', data=train, palette='coolwarm', alpha=0.7)
plt.title('Age vs Charges (Colored by Smoker Status)', fontsize=16)
plt.show()

"""
---

# üìà Interpretation: Age vs Charges (Smoker vs Non-Smoker)

This scatter plot shows the relationship between **age and insurance charges**, with colors indicating **smoker status**.

### ‚úÖ Key Observations:

* **Charges increase with age** (older individuals tend to pay more).
* **Smokers (usually red)** have **much higher charges** than non-smokers at almost every age.
* There is a **clear separation** between smoker and non-smoker groups.
* Some **very high charges** occur mostly in **smokers**, even at younger ages.

üëâ Conclusion: **Smoking has a stronger impact on charges than age alone.**

"""



"""# Pairplot for Numeric Features"""

numeric_cols = train.select_dtypes(include=['int64','float64']).columns
sns.pairplot(train[numeric_cols], diag_kind='kde', corner=True)
plt.suptitle('Pairwise Relationships Among Numeric Features', y=1.02, fontsize=16)
plt.show()

"""---

# üîç Interpretation: Pairwise Relationships Among Numeric Features

This code creates a **pairplot** to visually explore how all numeric features in the dataset relate to each other.

### ‚úÖ What it shows:

* **Scatterplots** between each pair of numeric variables (to see relationships/correlations).
* **KDE curves (smooth histograms)** on the diagonal to show each variable‚Äôs distribution.
* **`corner=True`** shows only the lower triangle (cleaner, less cluttered).

### ‚úÖ Why it‚Äôs useful:

* Helps detect **correlations** between variables.
* Reveals **patterns, clusters, or trends**.
* Shows **which features might influence each other**.
* Identifies **non-linear relationships or outliers**.

üëâ In summary: This plot provides a **complete visual overview of how all numeric variables interact**, helping in **feature selection and understanding the data structure**.

"""

train_corr = train.select_dtypes(include=[np.number])
train_corr.shape

"""Sure! Here is the code with a **Markdown heading, purpose, and explanation** in a clean and professional format:

---

## üîé Selecting Only Numeric Columns (for Correlation / Analysis)

### ‚úÖ Purpose:

To extract only the **numeric columns** (integers and floats) from the dataset.
This is useful because many statistical methods‚Äîsuch as **correlation, scaling, and modeling**‚Äîwork only with numeric data.
We also check the **shape** to see how many numeric features we are working with.

### ‚úÖ Code:

```python
train_corr = train.select_dtypes(include=[np.number])
train_corr.shape
```

### ‚úÖ Explanation:

* `train.select_dtypes(include=[np.number])`
  ‚Üí Selects only columns with numeric data types (e.g., int64, float64).
  ‚Üí This prepares the dataset for tasks like **correlation analysis or feature selection**.

* `train_corr.shape`
  ‚Üí Returns the number of **rows and numeric columns**.
  ‚Üí Helps us understand how many numeric features are available for further analysis.

‚úÖ This step is often used before generating a **correlation matrix or heatmap** to study relationships between features.

"""

from sklearn.preprocessing import LabelEncoder
import seaborn as sns
import matplotlib.pyplot as plt

# Initialize label encoder
le = LabelEncoder()

# Encode all categorical columns directly in the original DataFrame
for col in train.select_dtypes(include=['object']).columns:
    train[col] = le.fit_transform(train[col])

# Now calculate correlation
corr = train.corr()

# Plot the heatmap
plt.figure(figsize=(10,6))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap (After Label Encoding)', fontsize=14)
plt.show()

"""---

## üîÑ Encoding Categorical Data & Visualizing Correlation

### ‚úÖ Purpose:

Machine learning models and correlation analysis require **numeric data only**.
However, our dataset contains **categorical columns (object type)** such as gender, smoker, and region.
This code converts those categories into numbers using **Label Encoding**, then computes and visualizes the **correlation heatmap** to understand feature relationships.

---

### ‚úÖ Code:

```python
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
import matplotlib.pyplot as plt

# Initialize label encoder
le = LabelEncoder()

# Encode all categorical columns directly in the original DataFrame
for col in train.select_dtypes(include=['object']).columns:
    train[col] = le.fit_transform(train[col])

# Now calculate correlation
corr = train.corr()

# Plot the heatmap
plt.figure(figsize=(10,6))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap (After Label Encoding)', fontsize=14)
plt.show()
```

---

### ‚úÖ Explanation (Step-by-Step):

#### 1Ô∏è‚É£ Import Libraries

* `LabelEncoder` ‚Üí Converts categorical text labels into numeric values.
* `seaborn` & `matplotlib` ‚Üí Used for plotting the heatmap.

#### 2Ô∏è‚É£ Initialize Encoder

```python
le = LabelEncoder()
```

Creates an encoder object for transforming categories into numbers.

#### 3Ô∏è‚É£ Encode All Categorical Columns

```python
for col in train.select_dtypes(include=['object']).columns:
    train[col] = le.fit_transform(train[col])
```

* Automatically finds all `object` (categorical) columns.
* Replaces each category with a numeric label (e.g., male=1, female=0).

‚úÖ Now the dataset is **fully numeric**, ready for correlation and ML models.

#### 4Ô∏è‚É£ Compute Correlation

```python
corr = train.corr()
```

Calculates correlation between all numeric columns.

#### 5Ô∏è‚É£ Plot Heatmap

```python
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
```

* `annot=True` ‚Üí Displays correlation values.
* `cmap='coolwarm'` ‚Üí Shows positive (red) and negative (blue) relationships.
* Helps identify **strong predictors of the target variable (charges).**

---

### üéØ Why This is Important?

‚úÖ Converts text into model-friendly numeric values
‚úÖ Enables correlation analysis
‚úÖ Helps in **feature selection** and **multicollinearity detection**
‚úÖ Improves data understanding for **better ML model performance**

---

# üìä Correlation Analysis ‚Äî Medical Insurance Dataset

## üîç Heatmap Insights

| Feature Pair | Correlation (r) | Relationship Strength | Interpretation |
|---------------|----------------|------------------------|----------------|
| **smoker ‚Üî charges** | **0.67** | üü¢ **Strong Positive** | Smoking greatly increases insurance charges. Smokers pay much higher medical costs. |
| **age ‚Üî charges** | **0.53** | üü° **Moderate Positive** | Older individuals tend to have higher medical charges. Age impacts cost significantly. |
| **bmi ‚Üî charges** | **0.13** | üîµ **Weak Positive** | Higher BMI (body mass index) slightly increases insurance costs but not very strongly. |
| **children ‚Üî charges** | **0.16** | üîµ **Weak Positive** | Number of children doesn‚Äôt affect medical charges much. |
| **sex ‚Üî charges** | **0.01** | ‚ö™ **No Correlation** | Gender has almost no influence on medical charges. |
| **region ‚Üî charges** | **-0.04** | ‚ö™ **No Correlation** | Region does not significantly affect insurance charges. |

---

## üß† Key Takeaways

- Smoking is the most influential factor affecting medical charges.  
- Age also plays a strong role ‚Äî costs increase with age.  
- BMI and number of children have minor effects.  
- Sex and region are practically irrelevant in determining insurance cost.  

---

## üìà Conclusion

- Features like **smoker** and **age** are **highly important** for prediction models.  
- You can **focus feature engineering** on these two for better model accuracy.  
- Weakly correlated columns (**sex**, **region**) may contribute **little** to the target variable.

#üéØ Identifying the Most Important Features for the Target (charges)
"""

print("Find most important features relative to target")
corr = train_corr.corr()
corr.sort_values(['charges'], ascending=False, inplace=True)
corr.charges

"""---

## üéØ Identifying the Most Important Features for the Target (charges)

### ‚úÖ Purpose:

This code helps us **find which features are most strongly correlated with the target variable `charges`**.
By sorting correlations in descending order, we can **identify the most influential predictors**, which is crucial for:

‚úî Feature selection
‚úî Understanding data relationships
‚úî Building better ML models

---

### ‚úÖ Code:

```python
print("Find most important features relative to target")
corr = train_corr.corr()
corr.sort_values(['charges'], ascending=False, inplace=True)
corr.charges
```

---

### ‚úÖ Explanation (Step-by-Step):

#### 1Ô∏è‚É£ Display message

```python
print("Find most important features relative to target")
```

Simply prints a message to explain what we are doing.

#### 2Ô∏è‚É£ Calculate correlations

```python
corr = train_corr.corr()
```

Computes the **correlation matrix** for all numeric columns in the DataFrame `train_corr`.

#### 3Ô∏è‚É£ Sort features by their correlation with `charges`

```python
corr.sort_values(['charges'], ascending=False, inplace=True)
```

* Sorts the rows of the correlation matrix based on their correlation with `charges`.
* Highest positive correlations appear at the top.
* Negative correlations (if any) will appear at the bottom.

#### 4Ô∏è‚É£ Display the correlations with charges

```python
corr.charges
```

Shows the correlation value of each feature relative to `charges`.

---

### ‚úÖ Why This Matters:

‚ú® Helps you quickly spot **strong predictors**
‚ú® Useful for **feature engineering & model building**
‚ú® Eliminates **irrelevant or weak features**

---

# **Preparing the Data for Modeling**
"""

y = train['charges']
X = train.drop('charges', axis = 1)

"""#  Train-Test Split"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

"""

---

## üìå Splitting the Dataset into Training and Testing Sets

### ‚úÖ Purpose:

The goal of this code is to **divide the dataset into two parts:**

* **Training set (X_train, y_train):** Used to train the machine learning model.
* **Testing set (X_test, y_test):** Used to evaluate how well the trained model performs on unseen data.

This step prevents **overfitting** and ensures the model can **generalize to new data.**

---

### ‚úÖ Code:

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
```

---

### ‚úÖ Detailed Explanation (Line by Line)

#### 1Ô∏è‚É£ Import function

```python
from sklearn.model_selection import train_test_split
```

* Imports the `train_test_split` function from scikit-learn.
* This function automatically splits data into training and testing subsets.

#### 2Ô∏è‚É£ Split the data

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
```

* `X`: All independent (input) features.
* `y`: Target variable (what we want to predict, e.g., charges).
* `test_size=0.2`:

  * 20% of the data will be used for **testing**.
  * 80% of the data will be used for **training** (default split).

The function returns **four outputs**:

| Variable | Meaning                 |
| -------- | ----------------------- |
| X_train  | Training input features |
| X_test   | Testing input features  |
| y_train  | Training target values  |
| y_test   | Testing target values   |

---

### ‚úÖ Why This Step is Important

‚úÖ Prevents overfitting
‚úÖ Evaluates real-world performance
‚úÖ Ensures fair model comparison
‚úÖ Essential for any ML workflow

---

"""

y_test

"""## üßæ üßπ Feature Refinement: Removing Less Useful Columns

"""

train_copy = train.drop(columns=['sex', 'region'])

"""---

## üßπ Feature Refinement: Removing Less Useful Columns

### üéØ Purpose:

This step removes columns that may **not contribute significantly to the prediction task** or could introduce **noise or redundancy** in the model.
By dropping them, we simplify the dataset and improve model efficiency.

---

### ‚úÖ Code:

```python
train_copy = train.drop(columns=['sex', 'region'])
```

‚úÖ Creates a new DataFrame `train_copy`
‚úÖ Removes the columns `'sex'` and `'region'`
‚úÖ Keeps original `train` dataset unchanged for safety

---

# **ü§ñ Model Training & Evaluation Using Multiple Regressors
"""

# Step 1: Separate features and target
X = train_copy.drop(columns=['charges'])
y = train_copy['charges']

# Step 2: Train-test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Step 3: Encode categorical column 'smoker'
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
X_train['smoker'] = le.fit_transform(X_train['smoker'])
X_test['smoker'] = le.transform(X_test['smoker'])

# Step 4: Import models & metrics
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# (Optional) If XGBoost is installed, uncomment
# from xgboost import XGBRegressor

models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=42),
    "KNN Regressor": KNeighborsRegressor(n_neighbors=5),
    "Support Vector Regressor": SVR(),
    # "XGBoost Regressor": XGBRegressor(n_estimators=100, random_state=42)  # Optional
}

# Step 5: Train and evaluate each model
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    print(f"\n--- {name} ---")
    print("Accuracy (R¬≤ Score)      :", r2 * 100)
    print("Mean Absolute Error       :", mae)
    print("Mean Squared Error        :", mse)
    print("Root Mean Squared Error   :", rmse)

    # Example prediction
    pred_single = model.predict(X_test.iloc[[142]])[0]
    real_single = y_test.iloc[142]
    print(f"Prediction (142th row)   : {pred_single}")
    print(f"Real Value (142th row)   : {real_single}")

"""#7 Models and Display Metrics in a Table"""

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Dictionary of models
models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=42),
    "KNN Regressor": KNeighborsRegressor(n_neighbors=5),
    "Support Vector Regressor": SVR(),
    "Extra Tree Regressor": DecisionTreeRegressor(splitter='random', random_state=42)  # 7th model
}

# Empty list to store results
results = []

# Train and evaluate models
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Metrics
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)

    # Append to results list
    results.append({
        "Model": name,
        "R¬≤ Score (%)": r2 * 100,
        "MAE": mae,
        "MSE": mse,
        "RMSE": rmse
    })

# Create DataFrame
results_df = pd.DataFrame(results)

# Sort by best R¬≤ Score
results_df = results_df.sort_values(by="R¬≤ Score (%)", ascending=False)

# Display the table
print("üìä Model Performance Comparison:")
print(results_df)

"""---

# ‚úÖ üéØ Purpose of the Metrics Code

The purpose of this metrics code is to **evaluate the performance of each regression model** by calculating **4 important error/accuracy measures**:

‚úÖ R¬≤ Score ‚Üí How well the model explains the data (higher is better)
‚úÖ MAE (Mean Absolute Error) ‚Üí Average absolute difference (lower is better)
‚úÖ MSE (Mean Squared Error) ‚Üí Penalizes large errors more (lower is better)
‚úÖ RMSE (Root Mean Squared Error) ‚Üí Square root of MSE, same unit as target (lower is better)

üëâ After training each model, we use these metrics to compare all models and **find the most accurate model**.

---

# ‚úÖ üìå Step-by-Step Explanation of Metrics Code

```python
r2 = r2_score(y_test, y_pred)
```

‚úÖ **R¬≤ Score (Coefficient of Determination)**

* Tells how much variance in the target variable is explained by the model.
* Range: **-‚àû to 1**
* **Closer to 1 = Better performance**
* If R¬≤ = 0.85 ‚Üí Model explains 85% of data variation.

---

```python
mae = mean_absolute_error(y_test, y_pred)
```

‚úÖ **MAE (Mean Absolute Error)**

* Average absolute difference between actual and predicted values.
* Very easy to understand.
* **Lower is better.**
* Example: MAE = 1000 ‚Üí On average, the model is off by $1000.

---

```python
mse = mean_squared_error(y_test, y_pred)
```

‚úÖ **MSE (Mean Squared Error)**

* Squares the errors before averaging.
* Large mistakes = heavily penalized.
* **Very sensitive to outliers.**
* **Lower is better.**

---

```python
rmse = np.sqrt(mse)
```

‚úÖ **RMSE (Root Mean Squared Error)**

* Square root of MSE.
* Same units as the target variable.
* Easier to interpret than MSE.
* **Lower is better.**

---

# ‚úÖ üß† Why Use All 4 Metrics?

Each metric tells us something different:

| Metric   | What it Tells                        |
| -------- | ------------------------------------ |
| R¬≤ Score | Overall accuracy (higher = better)   |
| MAE      | Average error size                   |
| MSE      | Error with penalty on large mistakes |
| RMSE     | Realistic error in original units    |

‚úÖ Using all four gives a **complete performance picture**.

---

# ‚úÖ üîÅ Metrics Applied to All 7 Models

The code runs these calculations **inside a loop** for each model:

1. Train model
2. Predict test data
3. Calculate R¬≤, MAE, MSE, RMSE
4. Store results in a list
5. Convert to a DataFrame
6. Sort by best score (R¬≤)

This way, **all models are compared fairly**.

---

# ‚úÖ ‚úÖ Final Goal

üëâ Identify **which regression model is best** based on accuracy and error.
üëâ Use this information to **select the best model for predictions.**

---

---

# ‚úÖ Final Summary: Predicting Medical Insurance Charges

## üß† Problem Overview

We worked with a dataset of **1338 individuals**, containing the following features:

* Age
* Sex
* BMI (Body Mass Index)
* Number of children
* Smoking status
* Region
* Charges (Target variable)

üéØ **Goal:** Build machine learning models to accurately **predict medical insurance charges** based on personal and lifestyle features.

---

## üßπ Data Preparation Steps

‚úî Removed **low-impact features** (`sex`, `region`) after correlation analysis
‚úî Converted categorical column **smoker ‚Üí numeric** using Label Encoding
‚úî Split data into **training (80%)** and **testing (20%)** sets to evaluate models fairly

---

## ü§ñ Model Training ‚Äì 7 Regression Algorithms Used

We trained and compared **seven different regression models:**

1Ô∏è‚É£ Linear Regression ‚Äì Simple & interpretable
2Ô∏è‚É£ Decision Tree Regressor ‚Äì Rule-based, handles non-linearity
3Ô∏è‚É£ Random Forest Regressor ‚Äì Ensemble of trees, reduces overfitting
4Ô∏è‚É£ Gradient Boosting Regressor ‚Äì Sequential learning, high accuracy
5Ô∏è‚É£ KNN Regressor ‚Äì Predicts using nearest neighbors
6Ô∏è‚É£ Support Vector Regressor (SVR) ‚Äì Works well in complex spaces
7Ô∏è‚É£ (Optional) XGBoost Regressor ‚Äì Advanced boosting (if installed)

---

## üìä Metrics Used to Evaluate Each Model

To compare models fairly, we used **4 key performance metrics:**

‚úÖ **R¬≤ Score** ‚Äì How much variance is explained (higher is better)
‚úÖ **MAE (Mean Absolute Error)** ‚Äì Average absolute difference (lower = better)
‚úÖ **MSE (Mean Squared Error)** ‚Äì Penalizes large errors (lower = better)
‚úÖ **RMSE (Root MSE)** ‚Äì Real-world error in original units (lower = better)

---

## üèÜ Best Model: Gradient Boosting Regressor

| Metric             | Score                      |
| ------------------ | -------------------------- |
| R¬≤ Score           | **~85.8%** ‚úÖ               |
| MAE                | **0.206**                  |
| RMSE               | **0.362**                  |
| Prediction example | Very close to real value ‚úÖ |

üî• **Why it won?**
Gradient Boosting captures complex patterns and interactions between features, making it the most accurate model among all.

---

## üîé Key Insights from Analysis

* **Smoking** and **Age** are the **strongest predictors** of insurance charges.
* **BMI** and **number of children** have moderate influence.
* **Sex** and **region** contribute very little ‚Üí safely removed.
* Removing weak features boosted model performance.

---

## ‚úÖ Final Conclusion

We successfully built a **high-performing insurance cost prediction system** using multiple regression models and selected the **best one (Gradient Boosting Regressor)** based on evaluation metrics.

‚úÖ Accurate predictions
‚úÖ Meaningful feature importance
‚úÖ Efficient model for real-world use

üéØ This model can now be used to **estimate insurance charges for new individuals** with high confidence.


---
"""